{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "578aa877",
   "metadata": {},
   "source": [
    "# 오차역전파법\n",
    "- 가중치 매개변수의 기울기를 효율적으로 계산\n",
    "\n",
    "@ 오차역전파법을 제대로 이해하는 두 가지 방법\n",
    "1) 수식을 통해\n",
    "2) 계산 그래프를 통해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dee088",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프\n",
    "- 계산그래프 (computational graph)는 계산 과정을 그래프로 나타낸 것\n",
    "- 그래프는 복수의 노드(node)와 에지(edge)로 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79971ea9",
   "metadata": {},
   "source": [
    "### 5.1.1 계산 그래프로 풀다\n",
    "\n",
    "문제 : 현빈 군은 슈퍼에서 사과를 2개, 귤을 3개 샀다. 사과는 1개 100원, 귤은 1개에 150원, 소비세 10%일때 지불금액은?\n",
    "\n",
    "<img src='./1.JPG' >\n",
    "\n",
    "@ 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행됨\n",
    "- 계산 그래프를 구성한다.\n",
    "- 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. (순전파)\n",
    "- 순전파는 계산 그래프의 출발점부터 종착점으로의 전파\n",
    "- 역전파 : 종착점(오른쪽)에서 출발점(왼쪽)으로의 전파 (미분 계산시 중요 역할)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92975b",
   "metadata": {},
   "source": [
    "### 5.1.2 국소적 계산\n",
    "\n",
    "- 그래프의 특징은 국소적 계산을 전파함으로써 최종 결과를 얻음\n",
    "- 국소적 = 자신과 직접 관계된 작은 범위\n",
    "- 국소적 계산 = 전체에서 어떤 일이 벌어지든 상관 없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것\n",
    "- 계산 그래프는 각 노드에서 국소적 계산을 하므로 전체 계산이 아무리 복잡하더라도 단순하게 처리가능\n",
    "<img src='./2.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8dd14",
   "metadata": {},
   "source": [
    "### 5.1.3 왜 계산 그래프로 푸는가?\n",
    "> 계산 그래프의 이점\n",
    "\n",
    "- 국소적 계산\n",
    "- 중간 계산 결과를 모두 보관할 수 있음.\n",
    "- 계산 그래프는 '역전파'를 통해 미분을 효율적으로 계산할 수 있음\n",
    "<img src='./3.jpg' >\n",
    "\n",
    "@ 문제(현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀다. 이 때 지불금액은? (단, 소비세 10% 부과됨))를 예시로 한 설명\n",
    "- 역전파는 순전파와 반대 방향의 굵은 화살표 그림\n",
    "- 역전파는 국소적 미분 전달하고, 미분 값은 화살표의 아래에 표시\n",
    "- 사과가 1원 오르면 최종 금액은 2.2원 오른다는 의미\n",
    "- 소비세에 대한 지불 금액의 미분이나 사과 개수에 대한 지불 금액의 미분도 같은 순서로 구할 수 있음\n",
    "- 중간까지 구한 미분 결과 공유 가능하여 다수 미분 효율적 계산 가능\n",
    "- 계산 그래프의 이점은 순전파와 역전파를 활용해 각 변수의 미분을 효율적으로 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343efcf8",
   "metadata": {},
   "source": [
    "## 5.2 연쇄법칙\n",
    "- 국소적 미분을 전달하는 원리는 연쇄 법칙(chain rule)에 따른 것\n",
    "<img src='./4.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265b4cf7",
   "metadata": {},
   "source": [
    "### 5.2.1 계산 그래프의 역전파\n",
    "\n",
    "@ y = f(x)의 계산 역전파\n",
    "- 역전파의 계산 절차는 신호 E에 노드의 국소적 미분(ay/ax)을 곱한 후 다음노드로 전달\n",
    "- 국소적 미분은 순전파 때의 y = f(x) 계산의 미분을 구하는 것 즉, x에 대한 y의 미분을 구한다는 뜻\n",
    "- 국소적인 미분을 상류에서 전달된 값에 곱해 앞쪽 노드로 전달하는 것\n",
    "- 이 것이 역전파의 계산 순서, 왜 그런 일이 가능한지는 연쇄 법칙으로 설명가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ae62e",
   "metadata": {},
   "source": [
    "### 5.2.2 연쇄법칙이란?\n",
    "\n",
    "@ 합성 함수 : 여러 함수로 구성된 함수\n",
    "\n",
    "@ 연쇄 법칙은 합성 함수의 미분에 대한 성질이며, 다음과 같이 정의\n",
    "- 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
    "- 단,미분 전에 z = t^2 인 상태이이며, t = x + y라고 한다.\n",
    "\n",
    "> 이를 미분\n",
    "<img src='./5.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20eec38",
   "metadata": {},
   "source": [
    "### 5.2.3 연쇄법칙과 계산 그래프\n",
    "<img src='./6.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3ac86",
   "metadata": {},
   "source": [
    "## 5.3 역전파\n",
    "- 더하기, 곱하기 등의 연산을 예로 들어 역전파의 구조 살펴봄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da3ff2",
   "metadata": {},
   "source": [
    "### 5.3.1 덧셈 노드의 역전파\n",
    "\n",
    "<img src='./7.jpg' >\n",
    "\n",
    "- 덧셈 노드의 역전파는 입력값을 그대로 흘려보냄\n",
    "<img src='./8.jpg' >\n",
    "- 상류에서 전해진 미분값이 aL / az인 이유는 최종적으로 L이라는 값을 출력하는 큰 계산 그래프를 가정하기 때문\n",
    "- 위의 z = x + y 계산은 큰 계산 그래프의 중간 어딘가에 존재한다고 가정했기 때문에, 이 계산 그래프 상류에서부터 aL / az가 전해졌다고 가정\n",
    "<img src='./9.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803e81b",
   "metadata": {},
   "source": [
    "### 5.3.2 곱셈 노드의 역전파\n",
    "\n",
    "<img src='./10.jpg' >\n",
    "\n",
    "- 곱셈노드 역전파는 상류의 값에 입력 신호들을 서로 바꾼 값을 곱해서 하류로 보냄\n",
    "- 덧셈과 달리 곱셈 노드 구현시에는 순전파의 입력 신호를 변수에 저장해 둠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d79173",
   "metadata": {},
   "source": [
    "### 5.3.3 사과 쇼핑의 예\n",
    "\n",
    "- 사과가격, 사과개수, 소비세라는 세 변수 각각이 최종 금액에 어떻게 영향을 주느냐\n",
    "- 사과가격에 대한 지불 금액의 미분 등 3가지 변수에 대한 지불금액의 미분 구하기\n",
    "- 사과가격 미분 2.2 / 사과 개수 미분 110 / 소비세 미분 200 만큼 최종금액에 영향\n",
    "- 사과 가격과 소비세의 단위가 다르므로 주의해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40e620",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기\n",
    "- 사과쇼핑 파이썬 구현\n",
    "- 곱셈노드 'MulLayer' / 덧셈 노드 'AddLayer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df982dca",
   "metadata": {},
   "source": [
    "### 5.4.1 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177445e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 구현\n",
    "class MulLayer:\n",
    "  # 초기화\n",
    "  def __init__(self):\n",
    "    self.x = None\n",
    "    self.y = None\n",
    " \n",
    "  # 순전파\n",
    "  def forward(self, x, y):\n",
    "    self.x = x\n",
    "    self.y = y\n",
    "    out = x * y\n",
    " \n",
    "    return out\n",
    " \n",
    "  # 역전파\n",
    "  def backward(self, dout):\n",
    "    dx = dout * self.y # x와 y를 바꾼다\n",
    "    dy = dout * self.x\n",
    " \n",
    "    return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bbc6d",
   "metadata": {},
   "source": [
    "- MulLayer 사용하여 사과구매 순전파 구현\n",
    "- forward()는 순전파, backward()는 역전파를 처리\n",
    "- __init__()에서는 인스턴스 변수인 x와 y를 초기화 > 이 두 변수는 순전파 시의 입력 값을 유지하기 위해 사용\n",
    "- forward()에서는 x와 y를 인수로 받고 두 값을 곱해서 반환\n",
    "- backward()에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 '서로 바꿔' 곱한 후 하류로 흘림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e1b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    " \n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    " \n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    " \n",
    "print(price)\n",
    " \n",
    "# 각 변수에 대한 미분 - backward()에서 구할 수 있음\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    " \n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aaff73",
   "metadata": {},
   "source": [
    "### 5.4.2 덧셈 계층\n",
    "\n",
    "- 파이썬 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c23bc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price: 715\n",
      "dApple: 2.2\n",
      "dApple_num: 110\n",
      "dOrange: 3.3000000000000003\n",
      "dOrange_num: 165\n",
      "dTax: 650\n"
     ]
    }
   ],
   "source": [
    "class AddLayer:\n",
    "  def __init__(self): # 덧셈 계층에는 초기화 필요없음\n",
    "    pass # pass : 아무 것도 하지 말라는 명령어\n",
    " \n",
    "  def forward(self, x, y):\n",
    "    out = x + y\n",
    "    return out\n",
    " \n",
    "  def backward(self, dout):\n",
    "    dx = dout * 1\n",
    "    dy = dout * 1\n",
    "    return dx, dy\n",
    "\n",
    "# 덧셈 계층과 곱셈 계층 활용해 사과문제 풀이\n",
    "# from layer_naive import *\n",
    " \n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    " \n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    " \n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
    "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
    " \n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
    " \n",
    "print(\"price:\", int(price))\n",
    "print(\"dApple:\", dapple)\n",
    "print(\"dApple_num:\", int(dapple_num))\n",
    "print(\"dOrange:\", dorange)\n",
    "print(\"dOrange_num:\", int(dorange_num))\n",
    "print(\"dTax:\", dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34963df0",
   "metadata": {},
   "source": [
    "- 필요한 계층을 만든 후 순전파 메소드인 forward()를 적절한 순서로 호출. 그런 다음 순전파와 반대 순서로 역전파 메소드인 backward()를 호출하면 원하는 미분이 도출됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e403bd74",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기\n",
    "- 활성화함수 ReLU와 Sigmoid 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c700e8",
   "metadata": {},
   "source": [
    "## 5.5.1 ReLU 계층\n",
    "\n",
    "@ ReLU 계층의 식\n",
    "<img src='./11.jpg' >\n",
    "\n",
    "@ x에 대한 y의 미분의 식\n",
    "<img src='./12.jpg' >\n",
    "\n",
    "- 순전파 때의 입력인 x가 0보다 크면 역전파는 상류의 값을 그래돌 하류로 흘림\n",
    "- 순전파 때 x가 0 이하면 역전파 때는 하류로 신호를 보내지 않음 (0을 보냄)\n",
    "\n",
    "@ ReLU 계층의 계산 그래프\n",
    "<img src='./13.jpg' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38cef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 계층 코드 구현\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    " \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    " \n",
    "        return out\n",
    " \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    " \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a24b23",
   "metadata": {},
   "source": [
    "- Relu 클래스는 mask라는 인스턴스 변수 가짐\n",
    "- mask는 True / False로 구성된 넘파이 배열\n",
    "- 순전파 입력인 x의 원소값이 0 이하면 True, 아니면 False\n",
    "- 역전파 때는 순전파 때 만들어둔 mask를 써서 mask의 원소가 True인 곳에는 상류에서 전파된 dout을 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c0688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "x = np.array( [[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560b491",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid 계층\n",
    "\n",
    "@ 시그모이드 함수의 식\n",
    "<img src='./14.jpg' >\n",
    "\n",
    "@ 시그모이드 함수 계산그래프\n",
    "<img src='./15.jpg' >\n",
    "\n",
    "- exp 노드는 y = exp(x) 계산 수행\n",
    "- / 노드는 y = 1/x 계산 수행\n",
    "\n",
    "@ sigmoid 계층 역전파 한단계씩 짚어보기\n",
    "1) / 노드 (y = 1/x)를 미분\n",
    "- ay/ax = -(1/x^2) = -y^2\n",
    "- 역전파 때는 상류에서 흘러온 값에 -y^2을 곱함\n",
    "- -y^2 = 순전파의 출력을 제곱한 후 마이너스를 붙인 값\n",
    "    \n",
    "2. +노드 : 상류값 여과없이 하류로 보냄\n",
    "\n",
    "3. exp 노드는 exp(x) 연산 수행\n",
    "- 미분 : ay/ax = exp(x)\n",
    "- 상류의 값에 순전파 때의 출력 (계산그래프에서는 exp(-x))을 곱해 하류로 전파\n",
    "    \n",
    "4. X 노드 : 순전파 때의 값을 서로 바꿔 곱\n",
    "- 예시에서는 -1 곱\n",
    "- 역전파의 최종출력인 (aL/ay)*y^2exp(-x)가 하류 노드로 전파\n",
    "- 역전파 최종출력은 순전파의 입력 x와 출력 y만으로 계산할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed5143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 파이썬 구현\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    " \n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    " \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    " \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7f862",
   "metadata": {},
   "source": [
    "- 순전파의 출력 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dff005b",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d553fad",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine 계층\n",
    "\n",
    "- 신경망의 순전파때 수행하는 행렬곱은 기하학에서 어파인 변환이라고 함 = Affine계층으로 구현\n",
    "\n",
    "@ Affine 계층의 계산 그래프\n",
    "<img src='./16.jpg' >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7582ae5",
   "metadata": {},
   "source": [
    "### 5.6.2 배치용 Affine 계층\n",
    "- 데이터 N개를 묶어 순전파 하는 경우\n",
    "\n",
    "@ 배치용 Affine 계층 계산그래프\n",
    "<img src='./17.jpg' >\n",
    "- 기존과 다른 부분은 X의 형상이 (N,2)로 변한 것\n",
    "- 편향 덧셈 주의 : 순전파 때의 편향 덧셈은 X*W에 대한 편향이 각 데이터에 더해짐\n",
    "- ex) N = 2일때, 편향은 그 두 데이터 각각의 계산결과에 더해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c55ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "# 순전파 때의 편향 덧셈\n",
    "X_dot_W = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    " \n",
    "print(X_dot_W)\n",
    "print()\n",
    "print(X_dot_W + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a54ee",
   "metadata": {},
   "source": [
    "- 순전파의 편향 덧셈이 각각의 데이터에 더해지기 때문에 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffaed6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 역전파 때의 편향 덧셈\n",
    "dY = np.array([[1,2,3],[4,5,6]])\n",
    "print(dY)\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4211ab34",
   "metadata": {},
   "source": [
    "- 예시에서는 데이터가 2개(N=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cede8",
   "metadata": {},
   "source": [
    "@ 편향의 역전파는 그 두 데이터에 대한 미분을 데이터마다 더해서 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b044d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine 구현\n",
    "class Affine:\n",
    "  def __init__(self, W, b):\n",
    "    self.W = W\n",
    "    self.b = b\n",
    "    self.x = None\n",
    "    self.dW = None\n",
    "    self.db = None\n",
    " \n",
    "  def forward(self, x):\n",
    "    self.x = x\n",
    "    out = np.dot(x, self.W) + self.b\n",
    " \n",
    "    return out\n",
    " \n",
    "  def backward(self, dout):\n",
    "    dx = np.dot(dout, self.W.T)\n",
    "    self.dW = np.dot(self.x.T, dout)\n",
    "    self.db = np.sum(dout, axis=0)\n",
    " \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b6dba",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss 계층\n",
    "\n",
    "- 딥러닝에서는 학습과 추론 두 가지가 있다. 일반적으로 추론일 때는 Softmax 계층(layer)을 사용하지 않는다. Softmax 계층 앞의 Affine 계층의 출력을 점수(score)라고 하는데, 딥러닝의 추론에서는 답을 하나만 예측하는 경우에는 가장 높은 점수만 알면 되므로 Softmax 계층이 필요없다. 반면, 딥러닝을 학습할 때는 Softmax 계층이 필요하다.\n",
    "\n",
    "@ softmax 계층의 출력 그림\n",
    "<img src='./18.jpg' >\n",
    "- softmax 계층은 입력 값을 정규화 (출력의 합이 1이 되도록 변형)\n",
    "- softmax 계층과 손실함수 교차 엔트로피 포함하여 softmax-with-loss계층 구현\n",
    "\n",
    "@ softmax-with-loss 계층의 계산 그래프\n",
    "<img src='./19.jpg' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0de4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-with-Loss 계층 구현\n",
    "# common / layer.py\n",
    " \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실\n",
    "        self.y = None # softmax의 출력\n",
    "        self.t = None # 정답레이블 (원-핫 벡터)\n",
    " \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    " \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답레이블이 원-핫 벡터일 경우\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d037ec",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398adb8d",
   "metadata": {},
   "source": [
    "### 5.7.1 신경망 학습의 전체 그림\n",
    "\n",
    "> 전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라고 함\n",
    "\n",
    "- 미니배치 : 훈련 데이터 중 일부를 무작위로 가져옴 (= 미니배치)\n",
    "- 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함\n",
    "- 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "- 반복 : 1~3단계를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76deba3",
   "metadata": {},
   "source": [
    "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51e6c8",
   "metadata": {},
   "source": [
    "@ TwoLayerNet 클래스의 인스턴스 변수\n",
    "\n",
    "1) params\n",
    "- 딕셔너리 변수로, 신경망의 매개변수를 보관\n",
    "- params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향\n",
    "- params['W2']는 2번째 층의 편향\n",
    "\n",
    "2) layers\n",
    "- 순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관\n",
    "- layers['Affine1'], layers['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지\n",
    "\n",
    "3) lastLayer\n",
    "- 신경망의 마지막 계층\n",
    "- 이 예에서는 SoftmaxWithLoss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca114187",
   "metadata": {},
   "source": [
    "@ TwoLayerNet 클래스의 메소드\n",
    "\n",
    "1) __init__(self, input_size, hidden_size, output_size, weight_init_std)\n",
    "- 초기화를 수행한다.\n",
    "- 인수는 앞에서부터 입력층 뉴런 수, 은닉층 뉴런 수, 출력층 뉴런 수, 가중치 초기화 시 정규분포의 스케일\n",
    "\n",
    "2) predict(self, x)\n",
    "- 예측(추론)을 수행한다.\n",
    "- 인수 x는 이미지 데이터\n",
    "\n",
    "3) loss(self, x, t)\n",
    "- 손실 함수의 값을 구한다.\n",
    "- 인수 x는 이미지 데이터, t는 정답 레이블\n",
    "\n",
    "4) accuracy(self, x, t)\n",
    "- 정확도를 구한다.\n",
    "\n",
    "5) numerical_gradient(self, x, t)\n",
    "- 가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.\n",
    "\n",
    "6) gradient(self, x, t)\n",
    "- 가중치 매개변수의 기울기를 오차역전파법으로 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdecdfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    " \n",
    " \n",
    "class TwoLayerNet:\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    " \n",
    "        # 계층 생성\n",
    "        # OrderedDict = 순서가 있는 딕셔너리, 순서 기억\n",
    "        # 순전파 때는 계층을 추가한 순서대로 / 역전파 때는 계층 반대 순서로 호출\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    " \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x: 입력데이터, t : 정답레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x: 입력데이터, t : 정답레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward, 순전파\n",
    "        self.loss(x, t)\n",
    " \n",
    "        # backward, 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    " \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    " \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f08614",
   "metadata": {},
   "source": [
    "### 5.7.3 오차역전파법으로 구한 기울기 검증하기\n",
    "- 느린 수치 미분보다 오차역전파법을 사용한 해석적 방법이 효율적 계산 가능\n",
    "- 수치 미분은 오차역전파법을 정확히 구현했는지 확인 위해 필요\n",
    "- 수치 미분은 구현하기 쉬우나, 오차역전파법은 구현 복잡해 실수가 있을 수 있음\n",
    "- 기울기 확인(gradient check) : 두 방식으로 구한 기울기가 일치(거의 같음)함을 확인 하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9816a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.1759714494628753e-10\n",
      "b1:2.285552436708232e-09\n",
      "W2:6.232516010340499e-09\n",
      "b2:1.398754051931683e-07\n"
     ]
    }
   ],
   "source": [
    "# 기울기 확인 (gradient check)\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    " \n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    " \n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    " \n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    " \n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch) # 수치미분법\n",
    "grad_backprop = network.gradient(x_batch, t_batch) # 오차역전파법\n",
    " \n",
    "# 각 가중치 차이의 절댓값을 구한 후, 절댓값들의 평균 구함\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d3d4d",
   "metadata": {},
   "source": [
    "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3430c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10666666666666667 0.1052\n",
      "0.90055 0.9062\n",
      "0.9233833333333333 0.9268\n",
      "0.935 0.9356\n",
      "0.9463166666666667 0.945\n",
      "0.9515166666666667 0.9496\n",
      "0.9581666666666667 0.9539\n",
      "0.9600166666666666 0.9542\n",
      "0.9648833333333333 0.959\n",
      "0.96715 0.9601\n",
      "0.9696833333333333 0.9636\n",
      "0.97085 0.9644\n",
      "0.9734166666666667 0.9646\n",
      "0.9745666666666667 0.9672\n",
      "0.9745333333333334 0.9676\n",
      "0.9765666666666667 0.9666\n",
      "0.9776166666666667 0.9682\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    " \n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from ch05.two_layer_net import TwoLayerNet\n",
    " \n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    " \n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    " \n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    " \n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    " \n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    " \n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기 구함\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치미분법\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
